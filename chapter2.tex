В данной главе выпускной квалификационной работы будут приведены методы и инструменты, которые использовались для разработки и реализации модели рекомендации и плагина для платформы IntelliJ, который на основе модели будет рекомендовать забытые для модификации файлы на основе Git репозитория.

\section{Методы и инструменты, использованные для разработки и реализации модели рекомендации}\label{chapter2-models}
Перед началом выполнения работы следует разбить задачу, поставленную в пункте \ref{ml-model-req}, на подзадачи:
    \begin{itemize}
        \item проанализировать методы, которые могут быть использованы для решения поставленной задачи;
        \item разработать и обучить математические модели, решающие поставленную задачу;
        \item оценить качество полученных моделей на собранном наборе данных;
        \item сравнить качество полученных моделей с качеством существующих решений;
        \item если модель будет использоваться, как модель предсказания в плагине для IntelliJ платформы, то провести оценку качества модели на практике.
    \end{itemize}

В данной секции будут рассмотрены методы и инструменты, использованные для разработки и реализации модели рекомендаций. Всего было применено четыре метода: использование коэффициента Жаккара, нахождение ассоциативных правил, использование байесовского среднего, обучение и использование случайного леса. В главе три будут показаны результаты обучения и оценки качества моделей, использующих приведенные методы.
    \subsection{Мера Серенсена}\label{chapter-2-coef}
Самым простым методом, который использовался в данной работе, является метод, использующий коэффициент Жаккара, также известный, как коэффициент флористической общности \cite{jacard}. Этот индекс представляет из себя бинарную меру сходства сравниваемых объектов. Коэффициент Жаккара используется во многих областях наук, например, биоинформатике, геномике, биологии. 

Одной из модификаций приведенного коэффициента является мера Серенсена \cite{jacard2}. Именно данная мера и использовалась в работе. Выглядит она для конечных множеств следующим образом:
    $$K = \frac{n(A \cap B)}{n(A \cup B)}$$
В случае нашей задачи интерпретировать данную формулу следует следующим образом: пусть множество $A$ содержит в себе индексы коммитов, в которых присутствовал файл $a$, а множество $B$ содержит в себе индексы коммитов, в которых присутствовал файл $b$. $n(A \cap B)$ означает число коммитов, в которых коммитились и файл $a$, и файл $b$. Выражение $n(A \cup B)$, в свою очередь, означает число коммитов, в которых был или файл $a$, или файл $b$. Тогда коэффициент $K$ показывает то, насколько релевантен файл $a$ к файлу $b$. 

Таким образом, мы можем решать поставленную задачу в случае, когда множество $CommitFiles$ содержит один файл, назовем его $a$. Мы должны для всех остальных файлов в репозитории посчитать меру схожести Серенсена с файлом $a$, отсортировать их в порядке невозрастания и взять первые пять файлов. Так как у нас имеется набор данных, на котором мы можем подбирать некоторые гипер-параметры алгоритма, мы можем ввести здесь гипер-параметр, который назовем $threshold$. Также добавим этап перед сортировкой, который отфильтрует имеющиеся результаты подсчета коэффицентов, используя неравенство: 
    $$K > threshold$$
Подбирая приведенный гипер-параметр, мы сможем понизить число $FalsePositive$, тем самым увеличив метрику точности.

Следует заметить, что нашей задачей является поиск файлов, релевантных для некоторого множества файлов $CommitFiles$. Данное множество, конечно, может содержать более одного файла, и в таком случае для каждого файла из множества $CommitFiles$ проведем описанную выше процедуру без использования фильтрации и удаления файлов с малыми коэффициентами. Для каждого коэффициента у файла-кандидата возьмем среднее. Только теперь следует применить операцию фильтрации, и затем оставить пять файлов с наибольшей мерой Серенсена. В третьей главе будет проведена оценка качества данного решения.
    \subsection{Ассоциативные правила}\label{chapter2-rules}
Следующим подходом в решении поставленной в работе задачи является поиск и использование ассоциативных правил. Ассоциативные правила являются достаточно распространенным в анализе данных, состоящих из набора конечных множеств. Неформальным, но понятным объяснением того, как работают ассоциативные правила, является такой пример. Рассмотрим множество чеков, выданных в гипермаркете. В каждом чеке есть набор продуктов, которые покупатель приобрел. Стоит задача найти наиболее вероятные отношения такие, что из покупки множества продуктов $X$, следует покупка множества продуктов $Y$. Одним из таких отношений было найдено: вместе с хлебом покупатели с большой вероятностью покупали молоко \cite{as-rules}.
    \subsubsection{Использование ассоциативных правил}
Раскроем задачу, которую решает поиск ассоциативных правил, более формально. Пусть нам дано множество $Receipt \subseteq Item^k$, где $Item$ является натуральным числом~--- идентификатором продукта. Ассоциативным правилом называется импликация вида $X \Rightarrow Y$, где $X \subseteq Item^n$, $Y \subseteq Item^l$, $X \cap Y = \emptyset$. Поиск ассоциативных правил заключается в нахождении таких импликаций, что вероятность события $X$ и $Y$ находятся в одном $Receipt$. 

При внимательном рассмотрении, задача поиска ассоциативных правил можно спроецировать на задачу поставленную в работе. Заменим $Receipt$ на $Files$, $Item$ на $File$. Таким образом, ассоциативное правило будет означать, что какое-то множество файлов $X$ наиболее вероятно будет в одном коммите с множеством файлов $Y$. Предположим, мы получили ассоциативные правила для определенного репозитория. Чтобы сгенерировать рекомендацию для нового пользовательского коммита, найдем все такие правила, левая часть которых содержит подмножество файлов текущего коммита. Отфильтруем эти правила с помощью подобранного $threshold$, как в пункте \ref{chapter-2-coef}. Отсортируем по невозрастанию по вероятности каждого правила. Теперь объединим оставшиеся правые части с учетом порядка и возьмем первые пять файлов, которых не было в пользовательском коммите.
        \subsubsection{Алгоритм Apriori}\label{chapter2-apriori}
Для поиска ассоциативных правил обычно используется алгоритм, который называется Apriori \cite{apriori}. Для начала, введем два определения характеризующих ассоциативное правило: поддержка ($support$) и достоверность ($confidence$). Они вычисляются с использованием следующих формул:
    $$support(A \Rightarrow B) = \frac{n(A \cup B)}{N}$$
    $$confidence(A \Rightarrow B) = \frac{n(A \cup B)}{n(A)}$$
Если не использовать математическую нотацию, то поддержка~--- это вероятность встретить вместе элементы множеств $A$ и $B$, а достоверность~--- это вероятность встретить вместе элементы множеств $A$ и $B$ во множествах, где есть элементы $A$.

Перед началом работы алгоритма следует задать два гипер-параметра: минимальную поддержку и минимальную достоверность. 

Алгоритм Apriori работает следующим образом. В данном множестве $Receipts$ отсортируем каждый элемент. На полученном множестве построим суффиксное дерево \cite{trie}. Каждая вершина суффиксного дерева содержит в себе подмножество элементов из $Receipts$. Из корня построенного дерева запускается алгоритм поиска в ширину \cite{alghorithms}. Притом, алгоритм не опускается в вершины, поддержка которых меньше, чем установленная минимальная поддержка. Чтобы извлечь правила из вершин, множество в вершине разбивается на два (это и считается правилом) с учетом установленной минимальной достоверности. Таким образом, перебираются не все варианты подмножеств, а только те, что удовлетворяют поставленным условиям на минимальность. С такими оптимизациями алгоритм работает намного быстрее полного перебора правил и подсчета их вероятностей \cite{apriori}. 

Стоит отметить, что данная модель обучается под каждый репозиторий отдельно, то есть нельзя выделить общие правила для всех репозиториев, так как файлы в большей степени в них различны. Еще одним недостатком данного решения может быть то, что хранение большого количества ассоциативных правил может потреблять большое количество оперативной памяти, что может привести к невыполнению поставленной задачи.

В третьей главе будет оценено качество обученной модели на основе алгоритма Apriori на собранном наборе данных.
    \subsection{Байесовское среднее}\label{chapter-2-bayes}
Следующим методом решения поставленной задачи было обучение модели байесовской среднего \cite{bayesian-average}. Данная модель часто используется сайтами предоставляющими топ фильмов, основанных на оценках пользователей. Модель представляет из себя простую с математической точки зрения формулу:
    \begin{equation}\label{bayes-formula}
        W = v\frac{R}{m + v} + m\frac{C}{m + v}    
    \end{equation}

    \begin{itemize}
        \item $W$~--- итоговая оценка,
        \item $v$~--- количество голосов за фильм,
        \item $m$~--- вес, основанный на распределении рейтингов среди фильмов,
        \item $R$~--- средний рейтинг фильма,
        \item ${C}$~--- средняя оценка по всем фильмам.
    \end{itemize}

Для решения задачи поставленной в секции \ref{requirements} следует спроецировать модель байесовского среднего на термины из первой главы. В случае поставленной задачи файлы из пользовательского коммита будут <<голосовать>> за другие файлы в репозитории, притом величина голоса будет функцией-гипер-параметром. Будем использовать формулу \ref{bayes-formula}, но переменные в данной формуле будут означать:
\begin{itemize}
    \item $W$~--- оценка файла,
    \item $v$~--- число голосов за то, чтобы предсказать файл,
    \item $m$~--- минимальное число голосов за файл,
    \item $R$~--- средняя оценка для файла за предыдущие голосования,
    \item ${C}$~--- средняя оценка для всех файлов.
\end{itemize}

Алгоритм рекомендации будет выглядеть следующим образом: каждый файл из коммита голосует за остальные файлы из репозитория, не содержащихся в текущем коммите, функция голоса является гипер-параметром, голоса пересчитываются с использованием формулы \ref{bayes-formula}; далее стандартная процедура из пункта \ref{chapter-2-coef}~--- отфильтруем по подобранному $threshold$, отсортируем полученные оценки по невозрастанию, возьмем первые пять файлов или меньше, если их осталось меньше после фильтрации.

Стоит отметить, что данная модель практически не потребляет оперативную память сама по себе. Все расходы памяти будут приходиться на хранение истории.

В третьей главе будет оценено качество обученной модели на основе байесовского среднего на собранном наборе данных.
    \subsection{Случайный лес}\label{chapter2-forest}
Последний метод, который использовался в рамках выпускной квалификационной работы, это случайный лес. Перед тем, как определять, что такое случайный и как происходит его построение и обучение, следует ввести понятия дерева решений и бэггинга.

Дерево решений~--- это дерево, в вершинах (не листьях) которого содержится предикат на входных данных, а в листьях выходные данные модели (значения целевой функции). Для того, чтобы получить ответ на новые входные данные, следует пройти от корня дерева к листьям, притом каждый переход имеет вид: применить предикат, если он положительный, то идти в правого ребенка, если отрицательный, то в левого ребенка. Следует продолжать приведенную операцию, пока не дойдем до листа. Данные, хранящиеся в листе, и будут ответом модели на новые входные данные.

Бэггинг~--- это метод композиции нескольких моделей в одну. Каждая модель голосует за свое решение, те выходные данные, за которые проголосовала большая часть моделей, и будут выведены, как конечный ответ.

Случайный лес~--- это алгоритм машинного обучения, представляющий из себя бэггинг деревьев решений. Происходит он следующим образом. Если имеется выборка размера $N$, входные данные имеют $M$ признаков, задано число $k$ равное числу деревьев в лесе, а также зафиксировано число $m$, то для создания случайного леса требуется создать $k$ решающих деревьев, где каждое дерево создается по следующему алгоритму. Разбиваем с повторениями выходные данные на $N$ строк, на основе этих строк строим решающее дерево одним из известных алгоритмов построения \cite{decision-tree}. Важный момент, что разбиение вершин стоит производить на основе $m$ случайно выбранных признаков, не на всех $M$. Таким образом, у нас получается $k$ решающих деревьев, каждое из которых голосует за определенные выходные данные. В данной работе голоса деревьев будут учитываться с коэффициентом $\frac{1}{k}$.

Стоит отметить, что данная модель потребляет оперативную память в зависимости от количества деревьев и их высоты, от чего зависит и качество решающей функции, поэтому в третьей главе пришлось выбирать не самые лучшие в плане полученной оценки меры модели, так как потребление оперативной памяти даже на кратковременное хранение модели было сильно велико. Было решено хранить деревья в виде кода, как набор операций $if$, $else$, а лес в свою очередь, как множество таких файлов.

Для того, чтобы использовать случайный лес для решения поставленной задачи, следует свести ее к задаче бинарной классификации и выделить признаки, которые охарактеризуют текущий пользовательский коммит. Это будет сделано в третьей главе. Также, в третьей главе будет проведена оценка качества данной решающей функции.

\section{Методы и инструменты, использованные для разработки и реализации IntelliJ плагина}\label{chapter2-plugin-req}
Перед началом выполнения работы следует разбить задачу, поставленную в пункте \ref{impl-req}, на подзадачи:
    \begin{itemize}
        \item рассмотреть пути расширения функциональности IntelliJ платформы;
        \item проанализировать инструменты, которые помогут реализовать эффективный плагин;
        \item разработать и реализовать архитектуру плагина;
        \item добавить статистику, на основе которой можно будет сделать выводы о качестве решающей функции;
        \item выпустить реализованный плагин с одной из моделей в репозиторий плагинов (его можно найти по ссылке \cite{plugins-jetbrains});
        \item если плагин выполнит поставленные задачи, внедрить плагин в IDE на основе IntelliJ платформы.
    \end{itemize}
В данной секции будут рассмотрены элементы платформы IntelliJ, которые использовались при реализации плагина предсказания забытых для модификации файлов на основе Git репозитория.
    \subsection{IntelliJ платформа}\label{ij-platform-req}
Как уже говорилось в пункте \ref{chapter1-intellij}, платформа IntelliJ предоставляет возможность создания интегрированных сред разработки. Также, платформа позволяет расширять функциональность существующих IDE. Таким образом, интегрированные среды разработки, основанные на данной платформе, являются расширяемыми приложениями. Расширять можно разные части функциональности платформы, начиная от добавления простого действия добавлении $;$ в код, до полной поддержки языков программирования, например, существует плагин для поддержки языка Rust \cite{rust}. Платформа~--- это JVM (Java Virtual Machine) приложения, поэтому плагины тоже должны быть написаны на языках использующих JVM платформу. В данном пункте будет рассмотрено, каким образом можно расширять функциональность интегрированных сред разработки, основанных на платформе IntelliJ \cite{intellij-sdk}.

В рамках выпускной квалификационной работы использовалось три инструмента, с помощью которых можно расширять функциональность IDE: точки расширения, сервисы и стойкие компоненты.

Точка расширения~--- это интерфейс, соединяющий реализацию платформы и плагин. С помощью точек расширения платформа IntelliJ может использовать реализацию плагина. Более формально, точка расширения~--- это JVM интерфейс, который плагин может реализовать, платформа IntelliJ в свою очередь использует реализацию плагина с помощью методов данного интерфейса. Важным моментом в создании и реализации точек расширения, начиная с версии 2020.1 платформы, является создание и использование динамических точек расширения. Динамические плагины~--- это плагины, которые могут быть включены и выключены без перезагрузки IDE. Используемые точки расширения и их имплементации в плагине должны быть указаны в файле $plugin.xml$.

Сервис~--- это класс, который имеет ровно один объект, объект этого класса можно получить, вызвав специальный метод $getService()$ у $ServiceManager$. Сервисы бывают трех видов: уровня приложения, уровня проекта и уровня модуля. Для последних двух вызов метода $getService()$ создает один объект для соответствующей области действия. Важным моментом в реализации интерфейса является то, что сервис должен завершать процессы и очищать ссылки при отключении плагина~--- для этого реализуется интерфейс $Disposable$ и метод $dispose()$ у него. В противном случае, если процессы не завершены, плагин не сможет быть динамически выгружен из памяти. Созданные сервисы должны быть указаны в файле $plugin.xml$.

Стойкий компонент ($PersistentStateComponent$)~--- это сервис, который позволяет сохранять состояние на диск. В данной работе стойкие компоненты использовались для сохранения настроек пользователя, а именно состояния: должен плагин подсчитывать предсказания или нет.
    \subsection{VCS часть платформы}\label{chapter2-vcs}
Платформа состоит из многих частей, которые можно расширять. В рамка данной работы в основном использовалась VCS (Version Control System) часть платформы.

VCS модуль предоставляет возможность реализовать интеграцию для многих систем контроля версий, например, на основе данного модуля созданы реализации интеграция для Git, Mercurial, Perforce, Subversion. Использование части функциональности приведенных реализаций осуществляется через интерфейсы VCS модуля. В данной секции будут рассмотрены основные части VCS модуля, которые были использованы в выпускной квалификационной работе.

VCS модуль содержит точку расширения $CheckinHandler$, которая позволяет расширять функциональность, которая используется во время пользовательского коммита, приходящего из IDE. Она предоставляет информацию о текущем коммите, а также возможность производить произвольные действия во время пользовательского коммита, отменять коммит, изменять файлы в нем и так далее. Для выполнения поставленной задачи требуется использовать именно эту точку расширения, чтобы во время коммита показать пользователю рекомендации, сгенерированные моделью предсказаний.

Следующей важной частью VCS модуля является $VcsLog$ часть. Она предоставляет возможность эффективного получения истории репозитория. Для выполнения поставленной задачи во время пользовательского коммита требуется передать историю репозитория в модель, для эффективного ее получения следует использовать данную часть платформы. В четвертой главе более подробно рассказано использование этой части VCS модуля.
\section{Существующие решения задачи поиска релевантных файлов в Git репозитории}\label{git-also-anvaka-label}
На момент написания выпускной квалификационной работы было найдено единственное решение задачи, похожей на поставленную в данной работе, которое называется git-also\cite{git-also-anvaka}. Программа является приложением для командной строки. Она позволяет для заданного файла из репозитория получить набор файлов, с которыми введенный файл был в большем числе коммитов вместе. Данное решение не соответствует требованию, представленному в пункте \ref{impl-req}, т.к. приложение не является плагином для платформы IntelliJ. Также, данное приложение может решать задачу поставленную в пункте \ref{ml-model-req} только с использованием множества запросов. При внимательном рассмотрении данного решения, можно заметить, что оно использует решающую функцию, представленную в пункте \ref{chapter-2-coef}, а в секции \ref{chapter3-coef} было показано, что использование данной модели не позволяет выполнить задачу поставленную пункте \ref{ml-model-req}.
\chapterconclusion
В представленной главе были рассмотрены основные инструменты и методы, которые были использованы при создании модели и плагина предсказания забытых для модификации файлов на основе Git репозитория. Были рассмотрены алгоритмы машинного обучения, такие как ассоциативные правила, байесовское среднее, случайный лес. Было показано, как эти алгоритмы можно использовать для решения поставленной в пункте \ref{ml-model-req} задачи. В третьей главе будет показано качество данных решающих функций. После алгоритмов машинного обучения были изложены основные моменты, которые касаются создания плагина для IntelliJ платформы. Также были представлены основные компоненты, использование которых поможет выполнить задачу, поставленную в пункте \ref{impl-req}. В заключение, было проанализировано найденное решение задачи, похожей на поставленную в данной работе и объяснено, почему оно не удовлетворяет всем поставленным задачам.