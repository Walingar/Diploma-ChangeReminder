В данной главе выпускной квалификационной работы будут приведены методы и инструменты, которые использовались для разработки и реализации модели рекомендации и плагина для платформы IntelliJ, который на основе модели будет рекомендовать забытые для модификации файлы на основе Git репозитория.

\section{Методы и инструменты, использованные для разработки и реализации модели рекомендации}
В данной секции будут рассмотрены методы и инструменты, использованные для разработки и реализации модели рекомендаций. Всего было применено четыре метода: использование коэффицента Жаккара, нахождение ассоциативных правил, использование байесовского среднего, обучение и использование случайного леса. В главе три будут показаны результаты обучения и оценки качества моделей, использующих приведенные методы.
    \subsection{Мера Серенсена}\label{chapter-2-coef}
Самым простым методом, который использовался в данной работе, является метод, использующий коэффициент Жаккара, также известный, как коэффициент флористической общности \cite{jacard}. Этот индекс представляет из себя бинарную меру сходства сравниваемых объектов. Коэффициент Жаккара используется во многих областях наук, например, биоинформатике, геномике, биологии. 

Одной из модификаций приведенного коэффициента является мера Серенсена \cite{jacard2}. Именно данная мера и использовалась в работе. Выглядит она для конечных множеств следующим образом:
    $$K = \frac{n(A \cap B)}{n(A \cup B)}$$
В случае нашей задачи интерпретировать данную формулу следует следующим образом. Пусть множество $A$ содержит в себе индексы коммитов, в которых присутствовал файл $a$, а множество $B$ содержит в себе индексы коммитов, в которых присутствовал файл $b$. $n(A \cap B)$ означает число коммитов, в которых коммитились и файл $a$, и файл $b$. Выражение $n(A \cup B)$, в свою очередь, означает число коммитов, в которых был или файл $a$, или файл $b$. Тогда, коэффициент $K$ показывает то, насколько релевантен файл $a$ к файлу $b$. 

Таким образом, мы можем решать поставленную задачу в случае, когда множество $CommitFiles$ содержит один файл, назовем его $a$. Мы должны для всех остальных файлов в репозитории посчитать меру схожести Серенсена с файлом $a$, отсортировать их в порядке невозрастания, и взять первые пять файлов. Так как у нас имеется набор данных, на котором мы можем подбирать некоторые гипер-параметры алгоритма, мы можем ввести здесь гипер-параметр, который назовем $threshold$. И добавим этап перед сортировкой, который отфильтрует имеющиеся результаты подсчета коэффицентов, используя неравенство: 
    $$K > threshold$$
Подбирая приведенный гипер-параметр, мы сможем понизить число $FalsePositive$, тем самым увеличив метрику точности.

Следует заметить, что нашей задачей является поиск файлов, релевантных для некоторого множества файлов $CommitFiles$, данное множество, конечно, может содержать более одного файла. В таком случае, для каждого файла из множества $CommitFiles$ проведем описанную выше процедуру, без использования фильтрации и удаления файлов с малыми коэффицентами. Для каждого коэффицента у файла-кандидата возьмем среднее. И только теперь следует применить операцию фильтрации, затем оставить пять файлов с наибольшей мерой Серенсена. В третьей главе будет проведена оценка качества данного решения.
    \subsection{Ассоциативные правила}
Следующим подходом в решении поставленной в работе задачи является поиск и использование ассоциативных правил. Ассоциативные правила являются достаточно распространенным в анализе данных, состоящих из набора конечных множеств. Неформальным, но понятным объяснение того, как работают ассоциативные правила является такой пример. Рассмотрим множество чеков, выданных в гипермаркете. В каждом чеке есть набор продуктов, которые покупатель приобрел. Стоит задача найти наиболее вероятные отношения такие, что из покупки множества продуктов $X$, следует покупка множества продуктов $Y$. Одним из таких отношений было найдено: вместе с хлебом покупатели с большой вероятностью покупали молоко \cite{as-rules}.
    \subsubsection{Использование ассоциативных правил}
Раскроем задачу, которую решает поиск ассоциативных правил, более формально. Пусть нам дано множество $Receipt \subseteq Item^k$, где $Item$ является натуральным числом~--- идентификатором продукта. Ассоциативным правилом называется импликация вида $X \Rightarrow Y$, где $X \subseteq Item^n$, $Y \subseteq Item^l$, $X \cap Y = \emptyset$. Поиск ассоциативных правил заключается в нахождении таких импликаций, что вероятность события $X$ и $Y$ находятся в одном $Receipt$. 

При внимательном рассмотрении, задача поиска ассоциативных правил можно спроецировать на задачу поставленную в работе. Заменим $Receipt$ на $Files$, $Item$ на $File$. Таким образом, ассоциативное правило будет означать, что какое-то множество файлов $X$ наиболее вероятно будет в одном коммите с множеством файлов $Y$. Предположим, мы получили ассоциативные правила для определенного репозитория. Чтобы сгенерировать рекомендацию для нового пользовательского коммита, найдем все такие правила, левая часть которых содержит подмножество файлов текущего коммита. Отфильтруем эти правила с помощью подобранного $threshold$, как в пункте \ref{chapter-2-coef}. Отсортируем по невозрастанию по вероятности каждого правила. Теперь объединим оставшиеся правые части с учетом порядка. И возьмем первые пять файлов, которых не было в пользовательском коммите.
        \subsubsection{Алгоритм Apriori}
Для поиска ассоциативных правил обычно используется алгоритм, который называется Apriori \cite{apriori}. Для начала, введем два определения характеризующих ассоциативное правило: поддержка ($support$) и достоверность ($confidence$). Они вычисляются с использованием следующих формул:
    $$support(A \Rightarrow B) = \frac{n(A \cup B)}{N}$$
    $$confidence(A \Rightarrow B) = \frac{n(A \cup B)}{n(A)}$$
Если не использовать математическую нотацию, то поддержка~--- это вероятность встретить вместе элементы множеств $A$ и $B$. А достоверность~--- это вероятность встретить вместе элементы множеств $A$ и $B$, во множествах, где есть элементы $A$.

Перед началом работы алгоритма следует задать два гипер-параметра: минимальную поддержку и минимальную достоверность. 

Алгоритм Apriori работает следующим образом. В данном множестве $Receipts$ отсортируем каждый элемент. На полученном множестве построим суффиксное дерево \cite{trie}. Каждая вершина суффиксного дерева содержит в себе подмножество элементов из $Receipts$. Из корня построенного дерева запускается алгоритм поиска в ширину \cite{alghorithms}. Притом, алгоритм не опускается в вершины, поддержка которых меньшеч, чем установленная минимальная поддержка. Чтобы извлечь правила из вершин, множество в вершине разбивается на два, это и считается правилом, с учетом установленной минимальной достоверности. Таким образом, перебираются не все варианты подмножеств, а только те, что удовлетворяют поставленным условиям на минимальность, с такими оптимизациями алгоритм работает намного быстрее полного перебора правил и подсчета их вероятностей \cite{apriori}. 

Стоит отметить, что данная модель обучается под каждый репозиторий отдельно, то есть нельзя выделить общие правила для всех репозиториев, так как файлы в большей степени в них различны. Еще одним недостатком данного решения может быть то, что хранение большого количества ассоциативных правил может потреблять большое количество оперативной памяти, что может привести к невыполнению поставленной задачи.

В третьей главе будет оценено качество обученной модели на основе алгоритма Apriori на собранном наборе данных.
    \subsection{Байесовское среднее}\label{chapter-2-bayes}
Следующим методом решения поставленной задачи было обучение модели байесовской среднего \cite{bayesian-average}. Данная модель часто используется сайтами предоставляющими топ фильмов, основанных на оценках пользователей. Модель представляет из себя простую с математической точки зрения формулу:
    \begin{equation}\label{bayes-formula}
        W = v\frac{R}{m + v} + m\frac{C}{m + v}    
    \end{equation}

    \begin{itemize}
        \item $W$~--- итоговая оценка,
        \item $v$~--- количество голосов за фильм,
        \item $m$~--- вес, основанный на распределении рейтингов среди фильмов,
        \item $R$~--- средний рейтинг фильма,
        \item ${C}$~--- средняя оценка по всем фильмам.
    \end{itemize}

Для решения задачи поставленной в секции \ref{requirements} следует спроецировать модель байесовского среднего на термины из первой главы. В случае поставленной задачи файлы из пользовательского коммита будут <<голосовать>> за другие файлы в репозитории, притом величина голоса будет функцией-гипер-параметром. Будем использовать формулу \ref{bayes-formula}, но переменные в данной формуле будут означать:
\begin{itemize}
    \item $W$~--- оценка файла,
    \item $v$~--- число голосов за то, чтобы предсказать файл,
    \item $m$~--- минимальное число голосов за файл,
    \item $R$~--- средняя оценка для файла за предыдущие голосования,
    \item ${C}$~--- средняя оценка для всех файлов.
\end{itemize}

Алгоритм рекомендации будет выглядеть следующим образом: для каждый файл из коммита голосует за остальные файлы из репозитория, не содержащихся в текущем коммите, функция голоса является гипер-параметром, голоса пересчитываются с использованием формулы \ref{bayes-formula}, далее стандартная процедура из пункта \ref{chapter-2-coef}~--- отфильтруем по подобранному $threshold$, отсортируем полученные оценки по невозрастанию, возьмем первые пять файлов, или меньше, если и осталось меньше после фильтрации.

Стоит отметить, что данная модель практически не потребляет оперативную память сама по себе. Все расходы памяти будут приходится на хранение истории.

В третьей главе будет оценено качество обученной модели на основе байесовского среднего на собранном наборе данных.
    \subsection{Случайный лес}
Последний метод, который использовался в рамках выпускной квалификационной работы, это случайный лес. Перед тем, как определять, что такое случайный и как происходит его построение и обучение, следует ввести понятия дерева решений и бэггинга.

Дерево решений~--- это дерево, в вершинах (не листьях) которого содержится предикат на входных данных, а в листьях выходные данные модели (значения целевой функции). Для того, чтобы получить ответ на новые входные данные, следует пройти от корня дерева к листьям, притом каждый переход имеет вид: применить предикат, если он положительный, то идти в правого ребенка, если отрицательный, то в левого ребенка. Следует продолжать приведенную операцию, пока не дойдем до листа. Данные, хранящиеся в листе, и будут ответом модели на новые входные данные.

Бэггинг~--- это метод композиции нескольких моделей в одну. Каждая модель голосует за свое решение, те выходные данные, за которые проголосовала большая часть моделей, и будут выведены, как конечный ответ.

Случайный лес~--- это алгоритм машинного обучения, представляющий из себя бэггинг деревьев решений. Происходит он следующим образом. Если имеется выборка размера $N$, входные данные имеют $M$ признаков, задано число $k$ равное числу деревьев в лесе, а также зафиксировано число $m$, то для создания случайного леса требуется создать $k$ решающих деревьев, где каждое дерево создается по следующему алгоритму. Разбиваем с повторениями выходные данные на $N$ строк, на основе этих строк строим решающее дерево одним из известных алгоритмов построения \cite{decision-tree}. Важный момент, что разбиение вершин стоит производить на основе $m$ случайно выбранных признаков, не на всех $M$. Таким образом, у нас получается $k$ решающих деревьев, каждое из которых голосует за определенные выходные данные. В данной работе голоса деревьев будут учитываться с коэффицентом $\frac{1}{k}$.

Стоит отметить, что данная модель потребляет оперативную память в зависимости от количества деревьев и их высоты, от чего зависит и качество решающей функции, поэтому в третьей главе пришлось выбирать не самые лучшие в плане полученной оценки меры модели, так как потребление оперативной памяти даже на кратковременное хранение модели было сильно велико. Было решено хранить деревья в виде кода, как набор операций $if$, $else$, а лес в свою очередь, как множество таких файлов.

Для того, чтобы использовать случайный лес для решения поставленной задачи, следует свести ее к задаче бинарной классификации и выделить признаки, которые охарактеризуют текущий пользовательский коммит. Это будет сделано в третьей главе. Также, в третьей главе будет проведена оценка качества данной решающей функции.

\section{Методы и инструменты, использованные для разработки и реализации IntelliJ плагина}
    \subsection{IntelliJ платформа}
    \subsection{VCS часть платформы}
    \subsection{Плагин Git4Idea}
\section{Существующие решения задачи поиска релевантных файлов в Git репозитории}
На данный момент существует решение поставленной задачи, которое называется git-also. Программа является приложением для командной строки. Оно позволяет для заданного файла получить набор файлов, с которыми введенный файл был в большем числе коммитов вместе. Данное решение не соответствует требованию, представленному в пункте \ref{impl-req}, т.к. приложение не является плагином для платформы IntelliJ. Также, данное приложение может решать задачу поставленную в пункте \ref{ml-model-req} только с использованием множества запросов. При внимательном рассмотрении данного решения, можно заметить, что оно использует простую решающую функцию, которая будет являться для нас базовым показателем (см. пункт \ref{baseline}).
\chapterconclusion